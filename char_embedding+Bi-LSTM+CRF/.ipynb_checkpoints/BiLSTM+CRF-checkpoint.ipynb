{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = \"$UNK$\"\n",
    "NUM = \"$NUM$\"\n",
    "NONE = \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# shared global variables to be imported from model also\n",
    "UNK = \"$UNK$\"\n",
    "NUM = \"$NUM$\"\n",
    "NONE = \"O\"\n",
    "\n",
    "\n",
    "# special error message\n",
    "class MyIOError(Exception):\n",
    "    def __init__(self, filename):\n",
    "        # custom error message\n",
    "        message = \"\"\"\n",
    "ERROR: Unable to locate file {}.\n",
    "\n",
    "FIX: Have you tried running python build_data.py first?\n",
    "This will build vocab file from your train, test and dev sets and\n",
    "trimm your word vectors.\n",
    "\"\"\".format(filename)\n",
    "        super(MyIOError, self).__init__(message)\n",
    "\n",
    "\n",
    "class CoNLLDataset(object):\n",
    "    \"\"\"Class that iterates over CoNLL Dataset\n",
    "\n",
    "    __iter__ method yields a tuple (words, tags)\n",
    "        words: list of raw words\n",
    "        tags: list of raw tags\n",
    "\n",
    "    If processing_word and processing_tag are not None,\n",
    "    optional preprocessing is appplied\n",
    "\n",
    "    Example:\n",
    "        ```python\n",
    "        data = CoNLLDataset(filename)\n",
    "        for sentence, tags in data:\n",
    "            pass\n",
    "        ```\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, filename, processing_word=None, processing_tag=None,\n",
    "                 max_iter=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            filename: path to the file\n",
    "            processing_words: (optional) function that takes a word as input\n",
    "            processing_tags: (optional) function that takes a tag as input\n",
    "            max_iter: (optional) max number of sentences to yield\n",
    "\n",
    "        \"\"\"\n",
    "        self.filename = filename\n",
    "        self.processing_word = processing_word\n",
    "        self.processing_tag = processing_tag\n",
    "        self.max_iter = max_iter\n",
    "        self.length = None\n",
    "\n",
    "#读取数据\n",
    "    def __iter__(self):\n",
    "        niter = 0\n",
    "        with open(self.filename) as f:\n",
    "            words, tags = [], []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if (len(line) == 0 or line.startswith(\"-DOCSTART-\")):\n",
    "                    if len(words) != 0:\n",
    "                        niter += 1\n",
    "                        if self.max_iter is not None and niter > self.max_iter:\n",
    "                            break\n",
    "                        yield words, tags\n",
    "                        words, tags = [], []\n",
    "                else:\n",
    "                    ls = line.split(' ')\n",
    "                    word, tag = ls[0],ls[1]\n",
    "                    if self.processing_word is not None:\n",
    "                        word = self.processing_word(word)\n",
    "                    if self.processing_tag is not None:\n",
    "                        tag = self.processing_tag(tag)\n",
    "                    words += [word]\n",
    "                    tags += [tag]\n",
    "\n",
    "#计算长度？\n",
    "    def __len__(self):\n",
    "        \"\"\"Iterates once over the corpus to set and store length\"\"\"\n",
    "        if self.length is None:\n",
    "            self.length = 0\n",
    "            for _ in self:\n",
    "                self.length += 1\n",
    "\n",
    "        return self.length\n",
    "\n",
    "# 得到全部单词：输入：数据集的列表 \n",
    "# 输出： a set of word: 数据集里的所有单词 和 tags\n",
    "def get_vocabs(datasets):\n",
    "    \"\"\"Build vocabulary from an iterable of datasets objects\n",
    "\n",
    "    Args:\n",
    "        datasets: a list of dataset objects\n",
    "\n",
    "    Returns:\n",
    "        a set of all the words in the dataset\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Building vocab...\")\n",
    "    vocab_words = set()\n",
    "    vocab_tags = set()\n",
    "    for dataset in datasets:\n",
    "        for words, tags in dataset:\n",
    "            vocab_words.update(words)\n",
    "            vocab_tags.update(tags)\n",
    "    print(\"- done. {} tokens\".format(len(vocab_words)))\n",
    "    return vocab_words, vocab_tags\n",
    "\n",
    "# 得到单词的每个字母\n",
    "# 输入：可迭代的数据集的对象\n",
    "# 输出：单词的所有字母（set）\n",
    "def get_char_vocab(dataset):\n",
    "    \"\"\"Build char vocabulary from an iterable of datasets objects\n",
    "\n",
    "    Args:\n",
    "        dataset: a iterator yielding tuples (sentence, tags)\n",
    "\n",
    "    Returns:\n",
    "        a set of all the characters in the dataset\n",
    "\n",
    "    \"\"\"\n",
    "    vocab_char = set()\n",
    "    for words, _ in dataset:\n",
    "        for word in words:\n",
    "            vocab_char.update(word)\n",
    "\n",
    "    return vocab_char\n",
    "\n",
    "# glove里的所有单词\n",
    "def get_glove_vocab(filename):\n",
    "    \"\"\"Load vocab from file\n",
    "\n",
    "    Args:\n",
    "        filename: path to the glove vectors\n",
    "\n",
    "    Returns:\n",
    "        vocab: set() of strings\n",
    "    \"\"\"\n",
    "    print(\"Building vocab...\")\n",
    "    vocab = set()\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            word = line.strip().split(' ')[0]\n",
    "            vocab.add(word)\n",
    "    print(\"- done. {} tokens\".format(len(vocab)))\n",
    "    return vocab\n",
    "\n",
    "# 将glove里的所有单词都写进一个文件里，每个单词一行\n",
    "def write_vocab(vocab, filename):\n",
    "    \"\"\"Writes a vocab to a file\n",
    "\n",
    "    Writes one word per line.\n",
    "\n",
    "    Args:\n",
    "        vocab: iterable that yields word\n",
    "        filename: path to vocab file\n",
    "\n",
    "    Returns:\n",
    "        write a word per line\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Writing vocab...\")\n",
    "    with open(filename, \"w\") as f:\n",
    "        for i, word in enumerate(vocab):\n",
    "            if i != len(vocab) - 1:\n",
    "                f.write(\"{}\\n\".format(word))\n",
    "            else:\n",
    "                f.write(word)\n",
    "    print(\"- done. {} tokens\".format(len(vocab)))\n",
    "\n",
    "# 载入数据，输出一个字典\n",
    "def load_vocab(filename):\n",
    "    \"\"\"Loads vocab from a file\n",
    "\n",
    "    Args:\n",
    "        filename: (string) the format of the file must be one word per line.\n",
    "\n",
    "    Returns:\n",
    "        d: dict[word] = index\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        d = dict()\n",
    "        with open(filename) as f:\n",
    "            for idx, word in enumerate(f):\n",
    "                word = word.strip()\n",
    "                d[word] = idx\n",
    "\n",
    "    except IOError:\n",
    "        raise MyIOError(filename)\n",
    "    return d\n",
    "\n",
    "# 保存单词在glove里面的向量进入一个npz文件\n",
    "def export_trimmed_glove_vectors(vocab, glove_filename, trimmed_filename, dim):\n",
    "    \"\"\"Saves glove vectors in numpy array\n",
    "\n",
    "    Args:\n",
    "        vocab: dictionary vocab[word] = index\n",
    "        glove_filename: a path to a glove file\n",
    "        trimmed_filename: a path where to store a matrix in npy\n",
    "        dim: (int) dimension of embeddings\n",
    "\n",
    "    \"\"\"\n",
    "    embeddings = np.zeros([len(vocab), dim])\n",
    "    with open(glove_filename) as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(' ')\n",
    "            word = line[0]\n",
    "            embedding = [float(x) for x in line[1:]]#向量\n",
    "            if word in vocab:\n",
    "                word_idx = vocab[word]\n",
    "                embeddings[word_idx] = np.asarray(embedding)\n",
    "\n",
    "    np.savez_compressed(trimmed_filename, embeddings=embeddings)\n",
    "\n",
    "#载入文件\n",
    "def get_trimmed_glove_vectors(filename):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        filename: path to the npz file\n",
    "\n",
    "    Returns:\n",
    "        matrix of embeddings (np array)\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with np.load(filename) as data:\n",
    "            return data[\"embeddings\"]\n",
    "\n",
    "    except IOError:\n",
    "        raise MyIOError(filename)\n",
    "\n",
    "\n",
    "def get_processing_word(vocab_words=None, vocab_chars=None,\n",
    "                    lowercase=False, chars=False, allow_unk=True):\n",
    "    \"\"\"Return lambda function that transform a word (string) into list,\n",
    "    or tuple of (list, id) of int corresponding to the ids of the word and\n",
    "    its corresponding characters.\n",
    "\n",
    "    Args:\n",
    "        vocab: dict[word] = idx\n",
    "\n",
    "    Returns:\n",
    "        f(\"cat\") = ([12, 4, 32], 12345)\n",
    "                 = (list of char ids, word id)\n",
    "\n",
    "    \"\"\"\n",
    "    def f(word):\n",
    "        # 0. get chars of words\n",
    "        if vocab_chars is not None and chars == True:\n",
    "            char_ids = []\n",
    "            for char in word:\n",
    "                #f(word)\n",
    "                # ignore chars out of vocabulary\n",
    "                if char in vocab_chars:\n",
    "                    char_ids += [vocab_chars[char]]\n",
    "\n",
    "        # 1. preprocess word\n",
    "        if lowercase:\n",
    "            word = word.lower()\n",
    "        if word.isdigit():\n",
    "            word = NUM\n",
    "\n",
    "        # 2. get id of word\n",
    "        if vocab_words is not None:\n",
    "            if word in vocab_words:\n",
    "                word = vocab_words[word]\n",
    "            else:\n",
    "                if allow_unk:\n",
    "                    word = vocab_words[UNK]\n",
    "                else:\n",
    "                    raise Exception(\"Unknow key is not allowed. Check that \"\\\n",
    "                                    \"your vocab (tags?) is correct\")\n",
    "\n",
    "        # 3. return tuple char ids, word id\n",
    "        if vocab_chars is not None and chars == True:\n",
    "            return char_ids, word\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "    return f\n",
    "\n",
    "# padding and each sublist has same len\n",
    "def _pad_sequences(sequences, pad_tok, max_length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequences: a generator of list or tuple\n",
    "        pad_tok: the char to pad with\n",
    "\n",
    "    Returns:\n",
    "        a list of list where each sublist has same length\n",
    "    \"\"\"\n",
    "    sequence_padded, sequence_length = [], []\n",
    "\n",
    "    for seq in sequences:\n",
    "        seq = list(seq)\n",
    "        seq_ = seq[:max_length] + [pad_tok]*max(max_length - len(seq), 0)\n",
    "        sequence_padded +=  [seq_]\n",
    "        sequence_length += [min(len(seq), max_length)]\n",
    "\n",
    "    return sequence_padded, sequence_length\n",
    "\n",
    "# same as above, for the case we have characters ids\n",
    "def pad_sequences(sequences, pad_tok, nlevels=1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequences: a generator of list or tuple\n",
    "        pad_tok: the char to pad with\n",
    "        nlevels: \"depth\" of padding, for the case where we have characters ids\n",
    "\n",
    "    Returns:\n",
    "        a list of list where each sublist has same length\n",
    "\n",
    "    \"\"\"\n",
    "    if nlevels == 1:\n",
    "        max_length = max(map(lambda x : len(x), sequences))\n",
    "        sequence_padded, sequence_length = _pad_sequences(sequences,\n",
    "                                            pad_tok, max_length)\n",
    "\n",
    "    elif nlevels == 2:\n",
    "        max_length_word = max([max(map(lambda x: len(x), seq))\n",
    "                               for seq in sequences])\n",
    "        sequence_padded, sequence_length = [], []\n",
    "        for seq in sequences:\n",
    "            # all words are same length now\n",
    "            sp, sl = _pad_sequences(seq, pad_tok, max_length_word)\n",
    "            sequence_padded += [sp]\n",
    "            sequence_length += [sl]\n",
    "\n",
    "        max_length_sentence = max(map(lambda x : len(x), sequences))\n",
    "        sequence_padded, _ = _pad_sequences(sequence_padded,\n",
    "                [pad_tok]*max_length_word, max_length_sentence)\n",
    "        sequence_length, _ = _pad_sequences(sequence_length, 0,\n",
    "                max_length_sentence)\n",
    "\n",
    "    return sequence_padded, sequence_length\n",
    "\n",
    "# ?\n",
    "def minibatches(data, minibatch_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data: generator of (sentence, tags) tuples\n",
    "        minibatch_size: (int)\n",
    "\n",
    "    Yields:\n",
    "        list of tuples\n",
    "\n",
    "    \"\"\"\n",
    "    x_batch, y_batch = [], []\n",
    "    for (x, y) in data:\n",
    "        if len(x_batch) == minibatch_size:\n",
    "            yield x_batch, y_batch\n",
    "            x_batch, y_batch = [], []\n",
    "\n",
    "        if type(x[0]) == tuple:\n",
    "            x = zip(*x)\n",
    "        x_batch += [x]\n",
    "        y_batch += [y]\n",
    "\n",
    "    if len(x_batch) != 0:\n",
    "        yield x_batch, y_batch\n",
    "\n",
    "# BOI 分开\n",
    "def get_chunk_type(tok, idx_to_tag):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tok: id of token, ex 4\n",
    "        idx_to_tag: dictionary {4: \"B-PER\", ...}\n",
    "\n",
    "    Returns:\n",
    "        tuple: \"B\", \"PER\"\n",
    "\n",
    "    \"\"\"\n",
    "    tag_name = idx_to_tag[tok]\n",
    "    tag_class = tag_name.split('-')[0]\n",
    "    tag_type = tag_name.split('-')[-1]\n",
    "    return tag_class, tag_type\n",
    "\n",
    "# \n",
    "def get_chunks(seq, tags):\n",
    "    \"\"\"Given a sequence of tags, group entities and their position\n",
    "\n",
    "    Args:\n",
    "        seq: [4, 4, 0, 0, ...] sequence of labels\n",
    "        tags: dict[\"O\"] = 4\n",
    "\n",
    "    Returns:\n",
    "        list of (chunk_type, chunk_start, chunk_end)\n",
    "\n",
    "    Example:\n",
    "        seq = [4, 5, 0, 3]\n",
    "        tags = {\"B-PER\": 4, \"I-PER\": 5, \"B-LOC\": 3}\n",
    "        result = [(\"PER\", 0, 2), (\"LOC\", 3, 4)]\n",
    "\n",
    "    \"\"\"\n",
    "    default = tags[NONE]\n",
    "    idx_to_tag = {idx: tag for tag, idx in tags.items()}\n",
    "    chunks = []\n",
    "    chunk_type, chunk_start = None, None\n",
    "    for i, tok in enumerate(seq):\n",
    "        # End of a chunk 1\n",
    "        if tok == default and chunk_type is not None:\n",
    "            # Add a chunk.\n",
    "            chunk = (chunk_type, chunk_start, i)\n",
    "            chunks.append(chunk)\n",
    "            chunk_type, chunk_start = None, None\n",
    "\n",
    "        # End of a chunk + start of a chunk!\n",
    "        elif tok != default:\n",
    "            tok_chunk_class, tok_chunk_type = get_chunk_type(tok, idx_to_tag)\n",
    "            if chunk_type is None:\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "            elif tok_chunk_type != chunk_type or tok_chunk_class == \"B\":\n",
    "                chunk = (chunk_type, chunk_start, i)\n",
    "                chunks.append(chunk)\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # end condition\n",
    "    if chunk_type is not None:\n",
    "        chunk = (chunk_type, chunk_start, len(seq))\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.config import Config\n",
    "from model.data_utils import CoNLLDataset, get_vocabs, UNK, NUM, \\\n",
    "    get_glove_vocab, write_vocab, load_vocab, get_char_vocab, \\\n",
    "    export_trimmed_glove_vectors, get_processing_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Procedure to build data\n",
    "\n",
    "    You MUST RUN this procedure. It iterates over the whole dataset (train,\n",
    "    dev and test) and extract the vocabularies in terms of words, tags, and\n",
    "    characters. Having built the vocabularies it writes them in a file. The\n",
    "    writing of vocabulary in a file assigns an id (the line #) to each word.\n",
    "    It then extract the relevant GloVe vectors and stores them in a np array\n",
    "    such that the i-th entry corresponds to the i-th word in the vocabulary.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        config: (instance of Config) has attributes like hyper-params...\n",
    "\n",
    "    \"\"\"\n",
    "    # get config and processing of words\n",
    "    config = Config(load=False)\n",
    "    processing_word = get_processing_word(lowercase=True)\n",
    "\n",
    "    # Generators\n",
    "    dev   = CoNLLDataset(config.filename_dev, processing_word)\n",
    "    test  = CoNLLDataset(config.filename_test, processing_word)\n",
    "    train = CoNLLDataset(config.filename_train, processing_word)\n",
    "\n",
    "    # Build Word and Tag vocab\n",
    "    vocab_words, vocab_tags = get_vocabs([train, dev, test])\n",
    "    vocab_glove = get_glove_vocab(config.filename_glove)\n",
    "\n",
    "    vocab = vocab_words & vocab_glove\n",
    "    vocab.add(UNK)\n",
    "    vocab.add(NUM)\n",
    "\n",
    "    # Save vocab\n",
    "    write_vocab(vocab, config.filename_words)\n",
    "    write_vocab(vocab_tags, config.filename_tags)\n",
    "\n",
    "    # Trim GloVe Vectors\n",
    "    vocab = load_vocab(config.filename_words)\n",
    "    export_trimmed_glove_vectors(vocab, config.filename_glove,\n",
    "                                config.filename_trimmed, config.dim_word)\n",
    "\n",
    "    # Build and save char vocab\n",
    "    train = CoNLLDataset(config.filename_train)\n",
    "    vocab_chars = get_char_vocab(train)\n",
    "    write_vocab(vocab_chars, config.filename_chars)\n",
    "    print(\"done\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/trafficsign/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/miniconda3/envs/trafficsign/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/miniconda3/envs/trafficsign/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/miniconda3/envs/trafficsign/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/miniconda3/envs/trafficsign/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/miniconda3/envs/trafficsign/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/miniconda3/envs/trafficsign/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from model.data_utils import CoNLLDataset\n",
    "from model.ner_model import NERModel\n",
    "from model.config import Config\n",
    "from evaluate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # create instance of config\n",
    "    config = Config()\n",
    "\n",
    "    # build model\n",
    "    model = NERModel(config)\n",
    "    model.build()\n",
    "    # model.restore_session(\"results/crf/model.weights/\") # optional, restore weights\n",
    "    # model.reinitialize_weights(\"proj\")\n",
    "\n",
    "    # create datasets\n",
    "    dev   = CoNLLDataset(config.filename_dev, config.processing_word,\n",
    "                         config.processing_tag, config.max_iter)\n",
    "    train = CoNLLDataset(config.filename_train, config.processing_word,\n",
    "                         config.processing_tag, config.max_iter)\n",
    "\n",
    "    # train model\n",
    "    model.train(train, dev)\n",
    "    \n",
    "    #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # create instance of config\n",
    "    config = Config()\n",
    "\n",
    "    # build model\n",
    "    model = NERModel(config)\n",
    "    model.build()\n",
    "    model.restore_session(config.dir_model)\n",
    "\n",
    "    # create dataset\n",
    "    test  = CoNLLDataset(config.filename_test, config.processing_word,\n",
    "                         config.processing_tag, config.max_iter)\n",
    "\n",
    "    # evaluate and interact\n",
    "    model.evaluate(test)\n",
    "    interactive_shell(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/miniconda3/envs/trafficsign/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /opt/miniconda3/envs/trafficsign/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/caihaocheng/Desktop/A_UIC/FYPBackups/Coding/char_embedding+Bi-LSTM+CRF/model/ner_model.py:138: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /Users/caihaocheng/Desktop/A_UIC/FYPBackups/Coding/char_embedding+Bi-LSTM+CRF/model/ner_model.py:138: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/caihaocheng/Desktop/A_UIC/FYPBackups/Coding/char_embedding+Bi-LSTM+CRF/model/ner_model.py:143: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /Users/caihaocheng/Desktop/A_UIC/FYPBackups/Coding/char_embedding+Bi-LSTM+CRF/model/ner_model.py:143: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/miniconda3/envs/trafficsign/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /opt/miniconda3/envs/trafficsign/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/miniconda3/envs/trafficsign/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /opt/miniconda3/envs/trafficsign/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/caihaocheng/Desktop/A_UIC/FYPBackups/Coding/char_embedding+Bi-LSTM+CRF/model/ner_model.py:154: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /Users/caihaocheng/Desktop/A_UIC/FYPBackups/Coding/char_embedding+Bi-LSTM+CRF/model/ner_model.py:154: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "/opt/miniconda3/envs/trafficsign/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "Initializing tf session\n",
      "Reloading the latest trained model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/miniconda3/envs/trafficsign/lib/python3.5/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /opt/miniconda3/envs/trafficsign/lib/python3.5/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from results/test/model.weights/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from results/test/model.weights/\n",
      "Testing model over test set\n",
      "f1 89.94 - acc 97.79\n",
      "\n",
      "This is an interactive mode.\n",
      "To exit, enter 'exit'.\n",
      "You can enter a sentence like\n",
      "input> I love Paris\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input> I love Pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "O O    B-MISC \n",
      "I love Pairs  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input> I love China\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "O O    B-LOC \n",
      "I love China \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input> CPC guidance leads nation to prosperity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "B-ORG O        O     O      O  O          \n",
      "CPC   guidance leads nation to prosperity \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input> China's central bank said Tuesday the country's M2, a broad measure of money supply that covers cash in circulation and all deposits, rose 10.7 percent year-on-year to 212.55 trillion yuan (about $30.49 trillion) at the end of July.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "O       O       O    O    O       O   O         O   O O     O       O  O     O      O    O      O    O  O           O   O   O         O    O    O       O            O  O      O        O    O      O      O         O  O   O   O  B-LOC \n",
      "China's central bank said Tuesday the country's M2, a broad measure of money supply that covers cash in circulation and all deposits, rose 10.7 percent year-on-year to 212.55 trillion yuan (about $30.49 trillion) at the end of July. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input> China's central bank said Tuesday\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "O       O       O    O    O       \n",
      "China's central bank said Tuesday \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input> China's central bank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "O       O       O    \n",
      "China's central bank \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input> Alibaba's Zhang Yong tops Forbes China Best CEOs list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "B-ORG     I-ORG I-ORG O    B-ORG  I-ORG I-ORG O    O    \n",
      "Alibaba's Zhang Yong  tops Forbes China Best  CEOs list \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input> Ke Art Museum sits in Liantang town, Qingpu district, about one hour's drive from downtown Shanghai.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "B-LOC I-LOC I-LOC  O    O  B-LOC    O     B-ORG  O         O     O   O      O     O    O        B-LOC     \n",
      "Ke    Art   Museum sits in Liantang town, Qingpu district, about one hour's drive from downtown Shanghai. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input> China's central bank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "O       O       O    \n",
      "China's central bank \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input> China central bank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "B-LOC O       O    \n",
      "China central bank \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input> exit\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
